{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ImplicitGan.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrodenhagen/pycluster/blob/master/ImplicitGan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afb_nc9PdyL0",
        "outputId": "d634e7ad-6d54-4bc4-e663-4d3514936833"
      },
      "source": [
        "!git clone https://github.com/tomrodenhagen/Implicit-Renderer.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Implicit-Renderer'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i8ynz4h3Z8U"
      },
      "source": [
        "def get_logits_from_prob(probs, eps=1e-4):\r\n",
        "    ''' Returns logits for probabilities.\r\n",
        "    Args:\r\n",
        "        probs (tensor): probability tensor\r\n",
        "        eps (float): epsilon value for numerical stability\r\n",
        "    '''\r\n",
        "    probs = np.clip(probs, a_min=eps, a_max=1-eps)\r\n",
        "    logits = np.log(probs / (1 - probs))\r\n",
        "    return logits\r\n",
        "class DepthFunction(torch.autograd.Function):\r\n",
        "    ''' Depth Function class.\r\n",
        "    It provides the function to march along given rays to detect the surface\r\n",
        "    points for the OccupancyNetwork. The backward pass is implemented using\r\n",
        "    the analytic gradient described in the publication.\r\n",
        "    '''\r\n",
        "    @staticmethod\r\n",
        "    def run_Bisection_method(d_low, d_high, n_secant_steps, ray0_masked,\r\n",
        "                             ray_direction_masked, decoder, c, logit_tau):\r\n",
        "        ''' Runs the bisection method for interval [d_low, d_high].\r\n",
        "        Args:\r\n",
        "            d_low (tensor): start values for the interval\r\n",
        "            d_high (tensor): end values for the interval\r\n",
        "            n_secant_steps (int): number of steps\r\n",
        "            ray0_masked (tensor): masked ray start points\r\n",
        "            ray_direction_masked (tensor): masked ray direction vectors\r\n",
        "            decoder (nn.Module): decoder model to evaluate point occupancies\r\n",
        "            c (tensor): latent conditioned code c\r\n",
        "            logit_tau (float): threshold value in logits\r\n",
        "        '''\r\n",
        "        d_pred = (d_low + d_high) / 2.\r\n",
        "        for i in range(n_secant_steps):\r\n",
        "            p_mid = ray0_masked + d_pred.unsqueeze(-1) * ray_direction_masked\r\n",
        "            with torch.no_grad():\r\n",
        "                f_mid = decoder(p_mid, c, batchwise=False,\r\n",
        "                                only_occupancy=True) - logit_tau\r\n",
        "            ind_low = f_mid < 0\r\n",
        "            d_low[ind_low] = d_pred[ind_low]\r\n",
        "            d_high[ind_low == 0] = d_pred[ind_low == 0]\r\n",
        "            d_pred = 0.5 * (d_low + d_high)\r\n",
        "        return d_pred\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def run_Secant_method(f_low, f_high, d_low, d_high, n_secant_steps,\r\n",
        "                          ray0_masked, ray_direction_masked, decoder, c,\r\n",
        "                          logit_tau):\r\n",
        "       \r\n",
        "        ''' Runs the secant method for interval [d_low, d_high].\r\n",
        "        Args:\r\n",
        "            d_low (tensor): start values for the interval\r\n",
        "            d_high (tensor): end values for the interval\r\n",
        "            n_secant_steps (int): number of steps\r\n",
        "            ray0_masked (tensor): masked ray start points\r\n",
        "            ray_direction_masked (tensor): masked ray direction vectors\r\n",
        "            decoder (nn.Module): decoder model to evaluate point occupancies\r\n",
        "            c (tensor): latent conditioned code c\r\n",
        "            logit_tau (float): threshold value in logits\r\n",
        "        '''\r\n",
        "        d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\r\n",
        "        for i in range(n_secant_steps):\r\n",
        "            p_mid = ray0_masked + d_pred.unsqueeze(-1) * ray_direction_masked\r\n",
        "            with torch.no_grad():\r\n",
        "                f_mid = decoder(p_mid, c, batchwise=False,\r\n",
        "                                only_occupancy=True) - logit_tau\r\n",
        "            ind_low = f_mid < 0\r\n",
        "        \r\n",
        "            if ind_low.sum() > 0:\r\n",
        "                d_low[ind_low] = d_pred[ind_low]\r\n",
        "                f_low[ind_low] = f_mid[ind_low]\r\n",
        "            if (ind_low == 0).sum() > 0:\r\n",
        "               \r\n",
        "                d_high[ind_low == 0] = d_pred[ind_low == 0]\r\n",
        "                f_high[ind_low == 0] = f_mid[ind_low == 0]\r\n",
        "\r\n",
        "            d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\r\n",
        "        return d_pred\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def perform_ray_marching(ray0, ray_direction, decoder, c=None,\r\n",
        "                             tau=0.5, n_steps=[128, 129], n_secant_steps=8,\r\n",
        "                             depth_range=[0., 2.4], method='secant',\r\n",
        "                             check_cube_intersection=True, max_points=3500000, debug=True):\r\n",
        "        ''' Performs ray marching to detect surface points.\r\n",
        "        The function returns the surface points as well as d_i of the formula\r\n",
        "            ray(d_i) = ray0 + d_i * ray_direction\r\n",
        "        which hit the surface points. In addition, masks are returned for\r\n",
        "        illegal values.\r\n",
        "        Args:\r\n",
        "            ray0 (tensor): ray start points of dimension B x N x 3\r\n",
        "            ray_direction (tensor):ray direction vectors of dim B x N x 3\r\n",
        "            decoder (nn.Module): decoder model to evaluate point occupancies\r\n",
        "            c (tensor): latent conditioned code\r\n",
        "            tay (float): threshold value\r\n",
        "            n_steps (tuple): interval from which the number of evaluation\r\n",
        "                steps if sampled\r\n",
        "            n_secant_steps (int): number of secant refinement steps\r\n",
        "            depth_range (tuple): range of possible depth values (not relevant when\r\n",
        "                using cube intersection)\r\n",
        "            method (string): refinement method (default: secant)\r\n",
        "            check_cube_intersection (bool): whether to intersect rays with\r\n",
        "                unit cube for evaluation\r\n",
        "            max_points (int): max number of points loaded to GPU memory\r\n",
        "        '''\r\n",
        "        # Shotscuts\r\n",
        "        batch_size, n_pts, D = ray0.shape\r\n",
        "        device = ray0.device\r\n",
        "        logit_tau = get_logits_from_prob(tau)\r\n",
        "        n_steps = torch.randint(n_steps[0], n_steps[1], (1,)).item()\r\n",
        "\r\n",
        "        # Prepare d_proposal and p_proposal in form (b_size, n_pts, n_steps, 3)\r\n",
        "        # d_proposal are \"proposal\" depth values and p_proposal the\r\n",
        "        # corresponding \"proposal\" 3D points\r\n",
        "        d_proposal = torch.linspace(\r\n",
        "            depth_range[0], depth_range[1], steps=n_steps).view(\r\n",
        "                1, 1, n_steps, 1).to(device)\r\n",
        "        d_proposal = d_proposal.repeat(batch_size, n_pts, 1, 1)\r\n",
        "        \r\n",
        "        if check_cube_intersection:\r\n",
        "            d_proposal_cube, mask_inside_cube = \\\r\n",
        "                get_proposal_points_in_unit_cube(ray0, ray_direction,\r\n",
        "                                                 padding=0.1,\r\n",
        "                                                 eps=1e-6, n_steps=n_steps)\r\n",
        "            d_proposal[mask_inside_cube] = d_proposal_cube[mask_inside_cube]\r\n",
        "\r\n",
        "        p_proposal = ray0.unsqueeze(2).repeat(1, 1, n_steps, 1) + \\\r\n",
        "            ray_direction.unsqueeze(2).repeat(1, 1, n_steps, 1) * d_proposal\r\n",
        "      \r\n",
        "        # Evaluate all proposal points in parallel\r\n",
        "        with torch.no_grad():\r\n",
        "            val = torch.cat([(\r\n",
        "                decoder(p_split, c, only_occupancy=True) - logit_tau)\r\n",
        "                for p_split in torch.split(\r\n",
        "                    p_proposal.view(batch_size, -1, 3),\r\n",
        "                    int(max_points / batch_size), dim=1)], dim=1).view(\r\n",
        "                        batch_size, -1, n_steps)\r\n",
        "        # Create mask for valid points where the first point is not occupied\r\n",
        "        mask_0_not_occupied = val[:, :, 0] < 0\r\n",
        "        \r\n",
        "        # Calculate if sign change occurred and concat 1 (no sign change) in\r\n",
        "        # last dimension\r\n",
        "        sign_matrix = torch.cat([torch.sign(val[:, :, :-1] * val[:, :, 1:]),\r\n",
        "                                 torch.ones(batch_size, n_pts, 1).to(device)],\r\n",
        "                                dim=-1)\r\n",
        "        cost_matrix = sign_matrix * torch.arange(\r\n",
        "            n_steps, 0, -1).float().to(device)\r\n",
        "        # Get first sign change and mask for values where a.) a sign changed\r\n",
        "        # occurred and b.) no a neg to pos sign change occurred (meaning from\r\n",
        "        # inside surface to outside)\r\n",
        "        values, indices = torch.min(cost_matrix, -1)\r\n",
        "        mask_sign_change = values < 0\r\n",
        "        mask_neg_to_pos = val[torch.arange(batch_size).unsqueeze(-1),\r\n",
        "                              torch.arange(n_pts).unsqueeze(-0), indices] < 0\r\n",
        "\r\n",
        "        # Define mask where a valid depth value is found\r\n",
        "        mask = mask_sign_change & mask_neg_to_pos & mask_0_not_occupied\r\n",
        "\r\n",
        "        # Get depth values and function values for the interval\r\n",
        "        # to which we want to apply the Secant method\r\n",
        "        n = batch_size * n_pts\r\n",
        "        d_low = d_proposal.view(\r\n",
        "            n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\r\n",
        "                batch_size, n_pts)[mask]\r\n",
        "        f_low = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\r\n",
        "            batch_size, n_pts)[mask]\r\n",
        "        indices = torch.clamp(indices + 1, max=n_steps-1)\r\n",
        "        d_high = d_proposal.view(\r\n",
        "            n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\r\n",
        "                batch_size, n_pts)[mask]\r\n",
        "        f_high = val.view(\r\n",
        "            n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\r\n",
        "                batch_size, n_pts)[mask]\r\n",
        "       \r\n",
        "        ray0_masked = ray0[mask]\r\n",
        "        ray_direction_masked = ray_direction[mask]\r\n",
        "\r\n",
        "     \r\n",
        "        if c is not None and c.shape[-1] != 0:\r\n",
        "           c = c.unsqueeze(1).repeat(1, n_pts, 1)[mask]\r\n",
        "        # Apply surface depth refinement step (e.g. Secant method)\r\n",
        "        if method == 'secant' and mask.sum() > 0:\r\n",
        "            d_pred = DepthFunction.run_Secant_method(\r\n",
        "                f_low, f_high, d_low, d_high, n_secant_steps, ray0_masked,\r\n",
        "                ray_direction_masked, decoder, c, logit_tau)\r\n",
        "        elif method == 'bisection' and mask.sum() > 0:\r\n",
        "            d_pred = DepthFunction.run_Bisection_method(\r\n",
        "                d_low, d_high, n_secant_steps, ray0_masked,\r\n",
        "                ray_direction_masked, decoder, c, logit_tau)\r\n",
        "        else:\r\n",
        "            d_pred = torch.ones(ray_direction_masked.shape[0]).to(device)\r\n",
        "     \r\n",
        "        # for sanity\r\n",
        "        pt_pred = torch.ones(batch_size, n_pts, 3).to(device)\r\n",
        "        pt_pred[mask] = ray0_masked + \\\r\n",
        "            d_pred.unsqueeze(-1) * ray_direction_masked\r\n",
        "        # for sanity\r\n",
        "        d_pred_out = torch.ones(batch_size, n_pts).to(device)\r\n",
        "        d_pred_out[mask] = d_pred\r\n",
        "\r\n",
        "        return d_pred_out, pt_pred, mask, mask_0_not_occupied\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, *input):\r\n",
        "        ''' Performs a forward pass of the Depth function.\r\n",
        "        Args:\r\n",
        "            input (list): input to forward function\r\n",
        "        '''\r\n",
        "        (ray0, ray_direction, decoder, c, n_steps, n_secant_steps, tau,\r\n",
        "         depth_range, method, check_cube_intersection, max_points) = input[:11]\r\n",
        "\r\n",
        "        # Get depth values\r\n",
        "        with torch.no_grad():\r\n",
        "            d_pred, p_pred, mask, mask_0_not_occupied = \\\r\n",
        "                DepthFunction.perform_ray_marching(\r\n",
        "                    ray0, ray_direction, decoder, c, tau, n_steps,\r\n",
        "                    n_secant_steps, depth_range, method, check_cube_intersection,\r\n",
        "                    max_points)\r\n",
        "\r\n",
        "        # Insert appropriate values for points where no depth is predicted\r\n",
        "        d_pred[mask == 0] = 20\r\n",
        "        d_pred[mask_0_not_occupied == 0] = 0\r\n",
        "\r\n",
        "        # Save values for backward pass\r\n",
        "        ctx.save_for_backward(ray0, ray_direction, d_pred, p_pred, c)\r\n",
        "        ctx.decoder = decoder\r\n",
        "        ctx.mask = mask\r\n",
        "\r\n",
        "        return d_pred\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_output):\r\n",
        "        ''' Performs the backward pass of the Depth function.\r\n",
        "        We use the analytic formula derived in the main publication for the\r\n",
        "        gradients. \r\n",
        "        Note: As for every input a gradient has to be returned, we return\r\n",
        "        None for the elements which do no require gradients (e.g. decoder).\r\n",
        "        Args:\r\n",
        "            ctx (Pytorch Autograd Context): pytorch autograd context\r\n",
        "            grad_output (tensor): gradient outputs\r\n",
        "        '''\r\n",
        "        ray0, ray_direction, d_pred, p_pred, c = ctx.saved_tensors\r\n",
        "        decoder = ctx.decoder\r\n",
        "        mask = ctx.mask\r\n",
        "        eps = 1e-3\r\n",
        "\r\n",
        "        with torch.enable_grad():\r\n",
        "            p_pred.requires_grad = True\r\n",
        "            f_p = decoder(p_pred, c, only_occupancy=True)\r\n",
        "            f_p_sum = f_p.sum()\r\n",
        "            grad_p = torch.autograd.grad(f_p_sum, p_pred, retain_graph=True)[0]\r\n",
        "            grad_p_dot_v = (grad_p * ray_direction).sum(-1)\r\n",
        "\r\n",
        "            if mask.sum() > 0:\r\n",
        "                grad_p_dot_v[mask == 0] = 1.\r\n",
        "                # Sanity\r\n",
        "                grad_p_dot_v[abs(grad_p_dot_v) < eps] = eps\r\n",
        "                grad_outputs = -grad_output.squeeze(-1)\r\n",
        "                grad_outputs = grad_outputs / grad_p_dot_v\r\n",
        "                grad_outputs = grad_outputs * mask.float()\r\n",
        "\r\n",
        "            # Gradients for latent code c\r\n",
        "            if c is None or c.shape[-1] == 0 or mask.sum() == 0:\r\n",
        "                gradc = None\r\n",
        "            else:\r\n",
        "                gradc = torch.autograd.grad(f_p, c, retain_graph=True,\r\n",
        "                                            grad_outputs=grad_outputs)[0]\r\n",
        "\r\n",
        "            # Gradients for network parameters phi\r\n",
        "            if mask.sum() > 0:\r\n",
        "                # Accumulates gradients weighted by grad_outputs variable\r\n",
        "                grad_phi = torch.autograd.grad(\r\n",
        "                    f_p, [k for k in decoder.parameters()],\r\n",
        "                    grad_outputs=grad_outputs, retain_graph=True)\r\n",
        "            else:\r\n",
        "                grad_phi = [None for i in decoder.parameters()]\r\n",
        "\r\n",
        "        # Return gradients for c, z, and network parameters and None\r\n",
        "        # for all other inputs\r\n",
        "        out = [None, None, None, gradc, None, None, None, None, None,\r\n",
        "               None, None] + list(grad_phi)\r\n",
        "        return tuple(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jed_g6eIdyL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "004af1d7-34c3-480c-d7eb-b6e505b660ec"
      },
      "source": [
        "import os# %matplotlib inline\n",
        "# %matplotlib notebook\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Volumes\n",
        "from pytorch3d.transforms import so3_exponential_map\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras, \n",
        "    NDCGridRaysampler,\n",
        "    MonteCarloRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        "    RayBundle,\n",
        "    ray_bundle_to_ray_points,\n",
        ")\n",
        "\n",
        "# obtain the utilized device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    print(\n",
        "        'Please note that NeRF is a resource-demanding method.'\n",
        "        + ' Running this notebook on CPU will be extremely slow.'\n",
        "        + ' We recommend running the example on a GPU'\n",
        "        + ' with at least 10 GB of memory.'\n",
        "    )\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-feb9f54c851c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Data structures and functions for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVolumes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mso3_exponential_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from pytorch3d.renderer import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x0V1Lay-dyL7"
      },
      "source": [
        "# render_size describes the size of both sides of the # render_size describes the size of both sides of the \n",
        "# rendered images in pixels. Since an advantage of \n",
        "# Neural Radiance Fields are high quality renders\n",
        "# with a significant amount of details, we render\n",
        "# the implicit function at double the size of \n",
        "# target images.\n",
        "render_size = 32\n",
        "# Our rendered scene is centered around (0,0,0) \n",
        "# and is enclosed inside a bounding box\n",
        "# whose side is roughly equal to 3.0 (world units).\n",
        "volume_extent_world = 3.0\n",
        "\n",
        "# 1) Instantiate the raysamplers.\n",
        "\n",
        "# Here, NDCGridRaysampler generates a rectangular image\n",
        "# grid of rays whose coordinates follow the PyTorch3d\n",
        "# coordinate conventions.\n",
        "raysampler_grid = NDCGridRaysampler(\n",
        "    image_height=render_size,\n",
        "    image_width=render_size,\n",
        "    n_pts_per_ray=128,\n",
        "    min_depth=0.1,\n",
        "    max_depth=volume_extent_world,\n",
        ")\n",
        "\n",
        "# MonteCarloRaysampler generates a random subset \n",
        "# of `n_rays_per_image` rays emitted from the image plane.\n",
        "raysampler_mc = MonteCarloRaysampler(\n",
        "    min_x = -1.0,\n",
        "    max_x = 1.0,\n",
        "    min_y = -1.0,\n",
        "    max_y = 1.0,\n",
        "    n_rays_per_image=750,\n",
        "    n_pts_per_ray=64,\n",
        "    min_depth=0.1,\n",
        "    max_depth=volume_extent_world,\n",
        ")\n",
        "\n",
        "# 2) Instantiate the raymarcher.\n",
        "# Here, we use the standard EmissionAbsorptionRaymarcher \n",
        "# which marches along each ray in order to render\n",
        "# the ray into a single 3D color vector \n",
        "# and an opacity scalar.\n",
        "raymarcher = EmissionAbsorptionRaymarcher()\n",
        "\n",
        "# Finally, instantiate the implicit renders\n",
        "# for both raysamplers.\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
        ")\n",
        "renderer_mc = ImplicitRenderer(\n",
        "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MiOdDqKvdyL7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/docs/tutorials/utils/plot_image_grid.py\n",
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/docs/tutorials/utils/generate_cow_renders.py\n",
        "from plot_image_grid import image_grid\n",
        "from generate_cow_renders import generate_cow_renders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0Ux_tiVndyL8"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "!nvidia-smi\n",
        "print(\"[INFO] loading CIFAR-10 data\")\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MCqJOO0GdyL9"
      },
      "source": [
        "from torch import nn\n",
        "class HarmonicEmbedding(torch.nn.Module):\n",
        "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
        "        \"\"\"\n",
        "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
        "        the harmonic embedding layer converts each feature\n",
        "        in `x` into a series of harmonic features `embedding`\n",
        "        as follows:\n",
        "            embedding[..., i*dim:(i+1)*dim] = [\n",
        "                sin(x[..., i]),\n",
        "                sin(2*x[..., i]),\n",
        "                sin(4*x[..., i]),\n",
        "                ...\n",
        "                sin(2**self.n_harmonic_functions * x[..., i]),\n",
        "                cos(x[..., i]),\n",
        "                cos(2*x[..., i]),\n",
        "                cos(4*x[..., i]),\n",
        "                ...\n",
        "                cos(2**self.n_harmonic_functions * x[..., i])\n",
        "            ]\n",
        "            \n",
        "        Note that `x` is also premultiplied by `omega0` before\n",
        "        evaluting the harmonic functions.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer(\n",
        "            'frequencies',\n",
        "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tensor of shape [..., dim]\n",
        "        Returns:\n",
        "            embedding: a harmonic embedding of `x`\n",
        "                of shape [..., n_harmonic_functions * dim * 2]\n",
        "        \"\"\"\n",
        "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
        "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        n_neurons = 128\n",
        "        super(Decoder, self).__init__()\n",
        "        self.harmonic_embedding = HarmonicEmbedding()\n",
        "        self.decoder_occ = nn.Sequential(nn.Linear(3,n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons, n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons, n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons,1))\n",
        "        self.decoder_col = nn.Sequential(nn.Linear(360,n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons, n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons, n_neurons),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear( n_neurons,3))\n",
        "   \n",
        "  \n",
        "\n",
        "    def forward(self, p,c, only_occupancy=True, batchwise=False):\n",
        "        x_col = self.decoder_col(self.harmonic_embedding(p))\n",
        "        x_occ = self.decoder_occ(p)\n",
        "\n",
        "        x = torch.cat((x_occ, x_col), axis=-1)\n",
        "        eps = 1\n",
        "        norm_bound = 3\n",
        "        sigm = nn.Sigmoid()\n",
        "        occ =  x[..., 0]  - 1\n",
        "        norm = torch.norm(p, dim=-1)\n",
        "        weight = sigm((norm_bound - norm) * 10 )\n",
        "        \n",
        "        occ = occ * weight  +  (1-weight) * (sigm(-(norm_bound - norm + eps)*10 ) - 0.5) \n",
        "      \n",
        "        if only_occupancy:\n",
        "         \n",
        "\n",
        "          \n",
        "          return occ\n",
        "        else:\n",
        "      \n",
        "          return nn.Sigmoid()(x[...,1:]), occ \n",
        "    def sample_points(self, n_points):\n",
        "       \n",
        "        points = torch.rand((n_points,3)).to(device) * 6 -3\n",
        "        return self(points, only_occupancy=True,c=None), points      \n",
        "    def pretrain(self, n_iter =1000):\n",
        "      optim = torch.optim.Adam(self.parameters(), lr= 0.001)\n",
        "      for k in range(n_iter):\n",
        "        optim.zero_grad()\n",
        "        X = torch.rand((1000, 3),device=device) * 6 - 3\n",
        "        Y = torch.le(torch.norm(X, dim=-1), 1.5).float() * 2 - 1\n",
        "\n",
        "        pred = self(X, only_occupancy=True,c=None)\n",
        "        loss = torch.mean( (pred - Y) ** 2)\n",
        "        loss.backward()\n",
        "       \n",
        "        optim.step()\n",
        "      print(loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwKqop3_4NW5"
      },
      "source": [
        "from pytorch3d.renderer import (\r\n",
        "   \r\n",
        "    NDCGridRaysampler,\r\n",
        "   \r\n",
        ")\r\n",
        "\r\n",
        "decoder = Decoder().to(device)\r\n",
        "#decoder.pretrain()\r\n",
        "class renderer_implicit(nn.Module):\r\n",
        "  def __init__(self,res = (32,32)):\r\n",
        "    super().__init__()\r\n",
        "    self.n_samples = 500\r\n",
        "    self.n_steps = [256, 257]\r\n",
        "    self.calc_depth = DepthFunction.apply\r\n",
        "    self.n_secant_steps = 10\r\n",
        "    self.method = \"secant\"\r\n",
        "    self.check_cube_intersection = False\r\n",
        "    self.schedule_ray_sampling = False\r\n",
        "    self.max_points = 100000\r\n",
        "    self.depth_range = [0., 10]\r\n",
        "    self.tau = 0.5\r\n",
        "\r\n",
        "    self.raysampler_grid = NDCGridRaysampler(\r\n",
        "                                image_height=res[0],\r\n",
        "                                image_width=res[1],\r\n",
        "                                n_pts_per_ray=1,\r\n",
        "                                min_depth=0.1,\r\n",
        "                                max_depth=0.5,\r\n",
        "                            ).to(device)\r\n",
        "   \r\n",
        "  def forward(self, decoder, c, cameras):\r\n",
        "      ray_0, ray_direction, _ ,_  = self.raysampler_grid(cameras)\r\n",
        "      bz = ray_0.shape[0]\r\n",
        "      height = ray_0.shape[1]\r\n",
        "      width = ray_0.shape[2]\r\n",
        "      ray_0_flat = ray_0.view(bz,-1, 3)\r\n",
        "      ray_direction_flat = ray_direction.view(bz,-1, 3)\r\n",
        "      if not c is None:\r\n",
        "        c_expanded = c.unsqueeze(-1).expand(-1,ray_0_flat.shape[1], -1)\r\n",
        "      else:\r\n",
        "        c_expanded = None\r\n",
        "      depth = self.forward_rays(decoder,  ray_0_flat, ray_direction_flat, c = c_expanded)\r\n",
        "      depth_transformed = depth.view(bz, height, width, 1)\r\n",
        "      p = depth_transformed * ray_direction + ray_0 \r\n",
        "\r\n",
        "      color,o = decoder(p,c,only_occupancy=False)\r\n",
        "     \r\n",
        "      return color,depth_transformed\r\n",
        "                                \r\n",
        "  def forward_rays(self, decoder,  ray0, ray_direction, c = None):\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    inputs = [ray0, ray_direction, decoder, c, self.n_steps,\r\n",
        "                      self.n_secant_steps, self.tau, self.depth_range,\r\n",
        "                      self.method, self.check_cube_intersection,\r\n",
        "                      self.max_points] + [k for k in decoder.parameters()]  \r\n",
        "    d_hat = self.calc_depth(*inputs )\r\n",
        "    return d_hat\r\n",
        "   \r\n",
        "  \r\n",
        "\r\n",
        "def make_cameras(n_cameras):\r\n",
        "    target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=2)\r\n",
        "    logRs = torch.zeros(n_cameras, 3, device=device)\r\n",
        "    logRs[:, 1] = torch.linspace(-0.15, 0.15, n_cameras, device=device)\r\n",
        "    Rs = so3_exponential_map(logRs)\r\n",
        "    Ts = torch.zeros(n_cameras, 3, device=device)\r\n",
        "    Ts[:, 2] = 2.0\r\n",
        "    cameras = FoVPerspectiveCameras(\r\n",
        "            R=Rs, \r\n",
        "            T=Ts, \r\n",
        "            znear=target_cameras.znear[0],\r\n",
        "            zfar=target_cameras.zfar[0],\r\n",
        "            aspect_ratio=target_cameras.aspect_ratio[0],\r\n",
        "            fov=target_cameras.fov[0],\r\n",
        "            device=device,\r\n",
        "        )\r\n",
        "    return cameras\r\n",
        "\r\n",
        "target_cameras = make_cameras(2)\r\n",
        "\r\n",
        "R = renderer_implicit()\r\n",
        "\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "optim = torch.optim.Adam(decoder.parameters(),lr=0.001)\r\n",
        "\r\n",
        "\r\n",
        "for k in range(300):\r\n",
        "  optim.zero_grad()\r\n",
        "  d, o  = R(decoder, None, target_cameras)\r\n",
        "  occ,points = decoder.sample_points(1000)\r\n",
        "  \r\n",
        "  loss = torch.mean((o - 2.7)**2) + torch.mean((torch.norm(occ , dim=-1) )**2) \r\n",
        "  if k%30==0:\r\n",
        "    print(loss, float(o[0,16,16]))\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "    plt.imshow(o.cpu().detach().numpy()[0,:,:,0])\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d_9tRKFaU0Y"
      },
      "source": [
        "t = torch.tensor(([[0,0,0],[0,0,2]])).to(device).float()\r\n",
        "print(decoder(t,c=None) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sD7EzG40dyL-"
      },
      "source": [
        "import random\n",
        "def preprocess(images):\n",
        "  images = images / 255.0 \n",
        "  return images \n",
        "def sample_images(batch_size):\n",
        "  l = list(range(trainX.shape[0]))\n",
        "  samples = random.sample(l, batch_size)\n",
        "  images = np.stack([trainX[i] for i in samples])\n",
        "  \n",
        "  return torch.tensor(images, device = device)\n",
        "def sample_cams(target_cameras, batch_idx):\n",
        "   batch_cameras = FoVPerspectiveCameras(\n",
        "        R = target_cameras.R[batch_idx], \n",
        "        T = target_cameras.T[batch_idx], \n",
        "        znear = target_cameras.znear[batch_idx],\n",
        "        zfar = target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
        "        fov = target_cameras.fov[batch_idx],\n",
        "        device = device,\n",
        "    )\n",
        "   return batch_cameras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nl6CcCBkdyL_"
      },
      "source": [
        "def get_cameras(n_cams):\n",
        "    target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=1, azimuth_range=180)\n",
        "    logRs = torch.zeros(n_cams, 3, device=device)\n",
        "    logRs[:, 1] = torch.linspace(-0.15, 0.15, n_cams, device=device)\n",
        "    Rs = so3_exponential_map(logRs)\n",
        "    Ts = torch.zeros(n_cams, 3, device=device)\n",
        "    Ts[:, 2] = 2.7\n",
        "    cams = FoVPerspectiveCameras(\n",
        "            R=Rs, \n",
        "            T=Ts, \n",
        "            znear=target_cameras.znear[0],\n",
        "            zfar=target_cameras.zfar[0],\n",
        "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
        "            fov=target_cameras.fov[0],\n",
        "            device=device,\n",
        "        )\n",
        "    return cams\n",
        "\n",
        "\n",
        "\n",
        "def volumetric_function(\n",
        "            ray_bundle: RayBundle,**kwargs\n",
        "        ) :\n",
        "            # first convert the ray origins, directions and lengths\n",
        "            # to 3D ray point locations in world coords\n",
        "            rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
        "            x = rays_points_world.norm(dim=-1 )[:,16, 16,:]\n",
        "            print(x.shape)\n",
        "            plt.plot(x.cpu().detach().T)\n",
        "            plt.show()\n",
        "            # set the densities as an inverse sigmoid of the\n",
        "            # ray point distance from the sphere centroid\n",
        "            rays_densities = torch.sigmoid(\n",
        "                -100.0 * ( -0.5 + rays_points_world.norm(dim=-1, keepdim=True))\n",
        "            )\n",
        "            plt.plot(rays_densities.cpu().detach()[:,16, 16,0])\n",
        "            plt.show()\n",
        "            # set the ray features to RGB colors proportional\n",
        "            # to the 3D location of the projection of ray points\n",
        "            # on the sphere surface\n",
        "            rays_features = torch.nn.functional.normalize(\n",
        "                rays_points_world, dim=-1\n",
        "            ) * 0.5 + 0.5\n",
        "            \n",
        "            return rays_densities, rays_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFnMp_hGQGgw"
      },
      "source": [
        "# load the models\r\n",
        "!git clone https://github.com/csinva/gan-vae-pretrained-pytorch\r\n",
        "sys.path.append(\"./gan-vae-pretrained-pytorch/cifar10_dcgan\")\r\n",
        "from dcgan import Discriminator, Generator\r\n",
        "\r\n",
        "D = Discriminator(ngpu=1).eval()\r\n",
        "G = Generator(ngpu=1).eval()\r\n",
        "\r\n",
        "# load weights\r\n",
        "D.load_state_dict(torch.load('./gan-vae-pretrained-pytorch/cifar10_dcgan/weights/netD_epoch_199.pth'))\r\n",
        "G.load_state_dict(torch.load('./gan-vae-pretrained-pytorch/cifar10_dcgan/weights/netG_epoch_199.pth'))\r\n",
        "if torch.cuda.is_available():\r\n",
        "    D = D.cuda()\r\n",
        "    G = G.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNgDDpfvQVgO"
      },
      "source": [
        "z = torch.randn( (1,100,1,1), device=device)\r\n",
        "\r\n",
        "\r\n",
        "torch.manual_seed(4)\r\n",
        "def transform(image):\r\n",
        "  image = image - torch.min(image)\r\n",
        "  return image / torch.max(image)\r\n",
        "def inv_transform(image):\r\n",
        "  return (image - 0.5) * 2\r\n",
        "with torch.no_grad():\r\n",
        "  image = G(z) \r\n",
        "target_cameras = get_cameras(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3LvGDGXTdyMA"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "# import random# First move all relevant variables to the correct device.\n",
        "renderer_grid = renderer_grid.to(device)\n",
        "renderer_mc = renderer_mc.to(device)\n",
        "\n",
        "\n",
        "# Instantiate the radiance field model.\n",
        "decoder = Decoder().to(device)\n",
        "decoder.pretrain()\n",
        "n_perspectives = 8\n",
        "# Instantiate the Adam optimizer. We set its master learning rate to 1e-3.\n",
        "lr = 0.0001\n",
        "latent = Variable(torch.randn(size=(n_perspectives, 100, 1, 1), device=device) )\n",
        "opt = torch.optim.Adam(list(decoder.parameters()) + [latent] , lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "def get_color(m):\n",
        "  m1,m2 = m\n",
        "  c,o = (m1.split([3, 1], dim=-1)\n",
        "                    ) \n",
        "  return c \n",
        "\n",
        "# We sample 6 random cameras in a minibatch. Each camera\n",
        "# emits raysampler_mc.n_pts_per_image rays.\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 1\n",
        "n_iter = 10\n",
        "camera_front = sample_cams(target_cameras, [0])\n",
        "other_cams = sample_cams(target_cameras, random.sample([i+1 for i in range(len(target_cameras) - 1) ], n_perspectives) )\n",
        "renderer_implicit_ = renderer_implicit()\n",
        "#other_cams = sample_cams(target_cameras, [0,0] )\n",
        "for e in range(epochs):\n",
        "  loss = 0\n",
        "  for k in range(n_iter):\n",
        "        opt.zero_grad()\n",
        "        \n",
        "   \n",
        "        \n",
        "        rendered_image_front,o_front = renderer_implicit_(cameras=camera_front, decoder=decoder,c=None )\n",
        "        \n",
        "        rendered_images_side,o_side = renderer_implicit_(cameras=other_cams, decoder=decoder,c=None )\n",
        "       \n",
        "        images_side = G(latent)\n",
        "       \n",
        "        loss_side = torch.mean((rendered_images_side - transform(images_side.permute(0,2,3,1) ) ) ** 2 )\n",
        "        loss_front = torch.mean((rendered_image_front - transform(image.permute(0,2,3,1) ) ) ** 2 )\n",
        "        loss_depth = torch.mean((o_front- 2.7)**2) + torch.mean((o_side- 2.7)**2)\n",
        "       # loss_front = torch.mean((rendered_image_front - transform(image.permute(0,2,3,1) ) ) ** 2 )\n",
        "      \n",
        "        loss_complete = loss_front  + loss_side + loss_depth * 100\n",
        "        loss += float(loss_complete)\n",
        "        loss_complete.backward()\n",
        "        opt.step()\n",
        "  print(float(loss_front), float(loss_side), loss_depth)\n",
        "  print(rendered_image_front.shape )\n",
        "  print(o_front.mean())\n",
        "  print(o_side.mean())\n",
        "  f, axarr = plt.subplots(ncols=4)\n",
        "  axarr[0].imshow(rendered_image_front.cpu().detach().numpy()[0])\n",
        "  axarr[1].imshow(transform(image.permute(0,2,3,1)).cpu().detach().numpy()[0] ) \n",
        "  axarr[2].imshow(rendered_images_side.cpu().detach().numpy()[0] ) \n",
        "  axarr[3].imshow(rendered_images_side.cpu().detach().numpy()[1] ) \n",
        " \n",
        "  plt.show()\n",
        "  f, axarr = plt.subplots(ncols=4)\n",
        "  axarr[0].imshow(o_front.cpu().detach().numpy()[0][:,:,0])\n",
        " # axarr[1].imshow(transform(image.permute(0,2,3,1)).cpu().detach().numpy()[0] ) \n",
        "  axarr[2].imshow(o_side.cpu().detach().numpy()[0][:,:,0] ) \n",
        "  axarr[3].imshow(o_side.cpu().detach().numpy()[1][:,:,0] ) \n",
        " \n",
        "  plt.show()\n",
        "        \n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}